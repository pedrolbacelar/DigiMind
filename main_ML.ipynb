{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ydata_profiling as pp\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, Lasso\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn import svm\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import csv\n",
    "from dtwinpy.Digital_Twin import Digital_Twin\n",
    "from random import randrange\n",
    "from joblib import dump, load\n",
    "import time\n",
    "import os\n",
    "from interface_API import interfaceAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function that finds the part ID in the Json\n",
    "import json\n",
    "import csv\n",
    "json_path = './models/5s_determ/initial.json'\n",
    "\n",
    "def find_id(json_path,part_of_interest_id):\n",
    "    piece_found = False\n",
    "    queue_number_to_remaining_machines = {1: 4, 2: 3, 3: 2, 4: 2, 5: 1}\n",
    "    header = [\"queues_initial_conditions\", \"processing_time_machines\", \"part_of_interest_current_queue\", \"part_of_interest_current_position\", \"remaining_machines\", \"part_of_interest_rct\"]\n",
    "    with open(json_path,'r') as json_file:\n",
    "        json_load = json.load(json_file)\n",
    "    part_of_interest = \"Part \" + str(part_of_interest_id)\n",
    "    for index, init in enumerate(json_load['initial']):\n",
    "        if part_of_interest in init:\n",
    "            part_of_interest_current_queue = index + 1\n",
    "            part_of_interest_current_position = init.index(part_of_interest)\n",
    "            piece_found = True\n",
    "        else:\n",
    "            continue\n",
    "    if piece_found == False:\n",
    "         return 0,0,0, piece_found\n",
    "    remaining_machines = queue_number_to_remaining_machines[part_of_interest_current_queue]\n",
    "    queues_initial_conditions = [len(init) for init in json_load['initial']]\n",
    "    processing_time_machines = \"[11, 17, 60, 38, 10]\"\n",
    "    sample = (queues_initial_conditions, processing_time_machines, part_of_interest_current_queue, part_of_interest_current_position, remaining_machines, 0)\n",
    "    with open (\"test.csv\",'w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(header)\n",
    "            writer.writerow(sample)\n",
    "    return part_of_interest_current_position, part_of_interest_current_queue, queues_initial_conditions, piece_found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing for the machine learning model\n",
    "#load data\n",
    "def load_data(filename):\n",
    "    df = pd.read_csv(filename)\n",
    "    df.columns = ['queues_initial_conditions', 'processing_time_machines',\n",
    "       'part_of_interest_current_queue', 'part_of_interest_current_position',\n",
    "       'remaining_machines', 'part_of_interest_rct']\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "#solve initial data type issues\n",
    "def string_to_list(s):\n",
    "    # Remove the brackets from the string\n",
    "    s = s.strip('[]')\n",
    "    # Split the string on commas to create a list of substrings\n",
    "    substrings = s.split(',')\n",
    "    # Strip whitespace from each substring and convert to the appropriate data type\n",
    "    result = [eval(sub.strip()) for sub in substrings]\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "# preprocess of useless columns\n",
    "def process_string(df):\n",
    "    df['queues_initial_conditions'] = df['queues_initial_conditions'].apply(string_to_list)\n",
    "    df.drop('processing_time_machines', axis=1, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# generate report of files\n",
    "def make_report(df,filename):\n",
    "    profile = pp.ProfileReport(df,title=\"Report HTML\")\n",
    "    profile.to_file(f\"profile_of_data_{filename}.html\")\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "# process once more the columns, but do it only now because analysis needed it unprocessed\n",
    "def process_relative(df):\n",
    "    df_new = df.copy()\n",
    "    chain_lenght = len(df_new['queues_initial_conditions'][0])\n",
    "    df_new['remaining_machines'] = df['remaining_machines'] / chain_lenght\n",
    "    df_new['part_of_interest_current_queue'] = df['part_of_interest_current_queue'] / chain_lenght\n",
    "\n",
    "    return df_new\n",
    "\n",
    "\n",
    "# generates one column for each queue of initial conditions \n",
    "def process_queues(df):\n",
    "\n",
    "    df_new = df.copy()\n",
    "    for column in range(len(df_new['queues_initial_conditions'][0])):\n",
    "        lista_ = []\n",
    "        for position in df_new['queues_initial_conditions']:\n",
    "            lista_.append(position[column])\n",
    "        \n",
    "        df_new[f'queues_initial_conditions_{column}'] = lista_\n",
    "    df_new.drop('queues_initial_conditions', axis=1, inplace=True)\n",
    "\n",
    "    return df_new\n",
    "\n",
    "# preprocessing - standardizing of the columns to apply the regression\n",
    "def standardize_pipeline(df):\n",
    "    df_new = df.copy()\n",
    "\n",
    "    # Columns to be scaled\n",
    "    columns_to_scale = ['part_of_interest_current_position', 'queues_initial_conditions_0',\n",
    "                       'queues_initial_conditions_1', 'queues_initial_conditions_2',\n",
    "                       'queues_initial_conditions_3', 'queues_initial_conditions_4']\n",
    "\n",
    "    proprocess = make_pipeline(\n",
    "        StandardScaler(with_mean=True, with_std=True, copy=True),\n",
    "    )\n",
    "\n",
    "    # Fit and transform pipeline on specified columns\n",
    "    df_new[columns_to_scale] = proprocess.fit_transform(df_new[columns_to_scale])\n",
    "\n",
    "    return df_new\n",
    "\n",
    "# concat processing functions\n",
    "def load_n_process(filename, report=True):\n",
    "    #filename = input('insert filename of digital twin data: ')\n",
    "\n",
    "    if filename == '':\n",
    "        filename = 'database_2.csv'\n",
    "\n",
    "    df = load_data(filename)\n",
    "    df = process_string(df)\n",
    "    if report:\n",
    "        make_report(df,filename)\n",
    "    df = process_relative(df)\n",
    "    df = process_queues(df)\n",
    "    # ALTERED\n",
    "    df = standardize_pipeline(df)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the model and predict for a new point\n",
    "model = 'model.joblib'\n",
    "new_point = 'test.csv'\n",
    "def predict_ML(model,new_point):\n",
    "    clf = load(model)\n",
    "    df2 = load_n_process(new_point,report=False)\n",
    "    df2 = df2.drop('part_of_interest_rct',axis=1)\n",
    "    y_pred_2 = clf.predict(df2)\n",
    "    return y_pred_2[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_ML():\n",
    "    rct_ML = predict_ML('model.joblib','test.csv')\n",
    "    return rct_ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API object created for 'DigiMind'.\n",
      "Following data was PUT successfully to 'Indicator' aspect: [{\"_time\": \"2023-03-30T22:49:55.644381Z\", \"Mean_Absolute_Error\": 2.1, \"Prediction_Reliability\": 2.2}]\n",
      "Following data was PUT successfully to 'RCT' aspect: [{\"_time\": \"2023-03-30T22:49:59.724230Z\", \"Part_ID\": \"1\", \"Remaining_Cycle_Time\": 157}]\n",
      "Following data was PUT successfully to 'Zone' aspect: [{\"_time\": \"2023-03-30T22:50:00.160690Z\", \"Position\": 1, \"Queue_1\": 12, \"Queue_2\": 0, \"Queue_3\": 0, \"Queue_4\": 0}]\n",
      "2 1 [12, 0, 0, 0, 0] 156.755\n",
      "1 1 [11, 1, 0, 0, 0] 156.755\n",
      "Following data was PUT successfully to 'Indicator' aspect: [{\"_time\": \"2023-03-30T22:50:08.629881Z\", \"Mean_Absolute_Error\": 2.1, \"Prediction_Reliability\": 2.2}]\n",
      "Following data was PUT successfully to 'RCT' aspect: [{\"_time\": \"2023-03-30T22:50:09.015156Z\", \"Part_ID\": \"1\", \"Remaining_Cycle_Time\": 157}]\n",
      "Following data was PUT successfully to 'Zone' aspect: [{\"_time\": \"2023-03-30T22:50:14.991632Z\", \"Position\": 1, \"Queue_1\": 11, \"Queue_2\": 1, \"Queue_3\": 0, \"Queue_4\": 0}]\n",
      "0 1 [10, 1, 0, 0, 0] 156.755\n",
      "Following data was PUT successfully to 'Indicator' aspect: [{\"_time\": \"2023-03-30T22:50:18.468464Z\", \"Mean_Absolute_Error\": 2.1, \"Prediction_Reliability\": 2.2}]\n",
      "Following data was PUT successfully to 'RCT' aspect: [{\"_time\": \"2023-03-30T22:50:18.920287Z\", \"Part_ID\": \"1\", \"Remaining_Cycle_Time\": 157}]\n",
      "Following data was PUT successfully to 'Zone' aspect: [{\"_time\": \"2023-03-30T22:50:22.090523Z\", \"Position\": 1, \"Queue_1\": 10, \"Queue_2\": 1, \"Queue_3\": 0, \"Queue_4\": 0}]\n",
      "0 2 [9, 1, 1, 0, 0] 158.68\n",
      "Following data was PUT successfully to 'Indicator' aspect: [{\"_time\": \"2023-03-30T22:50:25.618193Z\", \"Mean_Absolute_Error\": 2.1, \"Prediction_Reliability\": 2.2}]\n",
      "Following data was PUT successfully to 'RCT' aspect: [{\"_time\": \"2023-03-30T22:50:26.068397Z\", \"Part_ID\": \"1\", \"Remaining_Cycle_Time\": 159}]\n",
      "Following data was PUT successfully to 'Zone' aspect: [{\"_time\": \"2023-03-30T22:50:32.085845Z\", \"Position\": 2, \"Queue_1\": 9, \"Queue_2\": 1, \"Queue_3\": 1, \"Queue_4\": 0}]\n",
      "0 3 [6, 3, 1, 0, 0] 127.50133333333332\n",
      "Following data was PUT successfully to 'Indicator' aspect: [{\"_time\": \"2023-03-30T22:50:35.592168Z\", \"Mean_Absolute_Error\": 2.1, \"Prediction_Reliability\": 2.2}]\n",
      "Following data was PUT successfully to 'RCT' aspect: [{\"_time\": \"2023-03-30T22:50:41.533733Z\", \"Part_ID\": \"1\", \"Remaining_Cycle_Time\": 128}]\n",
      "Following data was PUT successfully to 'Zone' aspect: [{\"_time\": \"2023-03-30T22:50:46.062415Z\", \"Position\": 3, \"Queue_1\": 6, \"Queue_2\": 3, \"Queue_3\": 1, \"Queue_4\": 0}]\n",
      "0 3 [4, 3, 1, 1, 0] 127.50133333333332\n",
      "Following data was PUT successfully to 'Indicator' aspect: [{\"_time\": \"2023-03-30T22:50:49.544708Z\", \"Mean_Absolute_Error\": 2.1, \"Prediction_Reliability\": 2.2}]\n",
      "Following data was PUT successfully to 'RCT' aspect: [{\"_time\": \"2023-03-30T22:50:53.399114Z\", \"Part_ID\": \"1\", \"Remaining_Cycle_Time\": 128}]\n",
      "Following data was PUT successfully to 'Zone' aspect: [{\"_time\": \"2023-03-30T22:50:53.815994Z\", \"Position\": 3, \"Queue_1\": 4, \"Queue_2\": 3, \"Queue_3\": 1, \"Queue_4\": 1}]\n",
      "0 3 [3, 3, 1, 1, 1] 127.50133333333332\n",
      "Following data was PUT successfully to 'Indicator' aspect: [{\"_time\": \"2023-03-30T22:50:57.290358Z\", \"Mean_Absolute_Error\": 2.1, \"Prediction_Reliability\": 2.2}]\n",
      "Following data was PUT successfully to 'RCT' aspect: [{\"_time\": \"2023-03-30T22:50:57.678945Z\", \"Part_ID\": \"1\", \"Remaining_Cycle_Time\": 128}]\n",
      "Following data was PUT successfully to 'Zone' aspect: [{\"_time\": \"2023-03-30T22:51:03.524182Z\", \"Position\": 3, \"Queue_1\": 3, \"Queue_2\": 3, \"Queue_3\": 1, \"Queue_4\": 1}]\n",
      "0 3 [3, 3, 2, 0, 1] 127.50133333333332\n",
      "Following data was PUT successfully to 'Indicator' aspect: [{\"_time\": \"2023-03-30T22:51:07.924768Z\", \"Mean_Absolute_Error\": 2.1, \"Prediction_Reliability\": 2.2}]\n",
      "Following data was PUT successfully to 'RCT' aspect: [{\"_time\": \"2023-03-30T22:51:08.372651Z\", \"Part_ID\": \"1\", \"Remaining_Cycle_Time\": 128}]\n",
      "Following data was PUT successfully to 'Zone' aspect: [{\"_time\": \"2023-03-30T22:51:08.852412Z\", \"Position\": 3, \"Queue_1\": 3, \"Queue_2\": 3, \"Queue_3\": 2, \"Queue_4\": 0}]\n",
      "0 5 [2, 4, 2, 1, 1] 14.034268774703555\n",
      "Following data was PUT successfully to 'Indicator' aspect: [{\"_time\": \"2023-03-30T22:51:12.261463Z\", \"Mean_Absolute_Error\": 2.1, \"Prediction_Reliability\": 2.2}]\n",
      "Following data was PUT successfully to 'RCT' aspect: [{\"_time\": \"2023-03-30T22:51:12.696753Z\", \"Part_ID\": \"1\", \"Remaining_Cycle_Time\": 14}]\n",
      "Following data was PUT successfully to 'Zone' aspect: [{\"_time\": \"2023-03-30T22:51:13.092957Z\", \"Position\": 5, \"Queue_1\": 2, \"Queue_2\": 4, \"Queue_3\": 2, \"Queue_4\": 1}]\n"
     ]
    }
   ],
   "source": [
    "#Function that calls the prediction\n",
    "def collect_features(json_path,part_of_interest_id):\n",
    "    #Search for the id_part\n",
    "    part_of_interest_current_position, part_of_interest_current_queue, queues_initial_conditions, track_part = find_id(json_path,part_of_interest_id)\n",
    "    # If the piece is found, call the ML model\n",
    "    if track_part == True:\n",
    "        rct_ML = main_ML()\n",
    "        return part_of_interest_current_position, part_of_interest_current_queue, queues_initial_conditions, track_part, rct_ML\n",
    "    else:\n",
    "        return part_of_interest_current_position, part_of_interest_current_queue, queues_initial_conditions, track_part, None\n",
    "\n",
    "def main(part_of_interest_id):\n",
    "    #Select the number of jsons that we are going to skip\n",
    "    json_step = 1\n",
    "    api = interfaceAPI()\n",
    "\n",
    "    #Call the function for the first json\n",
    "    json_initial = \"./models/initial.json\"\n",
    "    part_of_interest_current_position, part_of_interest_current_queue, queues_initial_conditions, _, rct_ML = collect_features(json_initial,part_of_interest_id)\n",
    "    #Send this data to the mindsphere\n",
    "    api.Indicator([2.1,2.2])\n",
    "    api.RCT([\"1\",round(rct_ML)])\n",
    "    api.Zone([part_of_interest_current_queue]+queues_initial_conditions)\n",
    "    print(part_of_interest_current_position, part_of_interest_current_queue, queues_initial_conditions, rct_ML)\n",
    "\n",
    "    #Sleep for a specific amount of time\n",
    "    time.sleep(json_step*3)\n",
    "\n",
    "    #Count the number of json files minus the first one\n",
    "    _, _, files = next(os.walk(\"./models\"))\n",
    "    number_jsons = len(files) - 1\n",
    "\n",
    "    #iterate for the remaining jsons\n",
    "    i = 0\n",
    "    while i < number_jsons:\n",
    "        check_piece = False\n",
    "        while check_piece == False and i < number_jsons:\n",
    "            # Check all the json files\n",
    "            json_path = \"./models/\" + files[i]\n",
    "            part_of_interest_current_position, part_of_interest_current_queue, queues_initial_conditions, check_piece, rct_ML = collect_features(json_path,part_of_interest_id)\n",
    "            i += 1\n",
    "        if check_piece == True:\n",
    "            #Send the data to mindsphere\n",
    "            print(part_of_interest_current_position, part_of_interest_current_queue, queues_initial_conditions, rct_ML)\n",
    "            api.Indicator([2.1,2.2])\n",
    "            api.RCT([\"1\",round(rct_ML)])\n",
    "            api.Zone([part_of_interest_current_queue]+queues_initial_conditions)\n",
    "        time.sleep(json_step*3)\n",
    "        i += json_step\n",
    "        #Sleep for a specific amount of time\n",
    "\n",
    "main(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6, 1, 2, 3, 4]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[6] + [1,2,3,4]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "djangodev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
